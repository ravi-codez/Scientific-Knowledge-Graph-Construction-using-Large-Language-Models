{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4764d4ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ravichoudhary/Desktop/graphusion_self_se/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "\n",
    "import nltk\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from umap import UMAP\n",
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from nltk.data import find\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from fuzzywuzzy import fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ea3e403",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"run_name\": \"test\",\n",
    "    \"dataset\": \"test\",\n",
    "    \"relation_definitions_file\": \"test/relation_types.json\",\n",
    "    \"input_json_file\": \"\",\n",
    "    \"input_triple_file\": \"\",\n",
    "    \"model\": \"gpt-3.5-turbo\",\n",
    "    \"max_resp_tok\": 200,\n",
    "    \"max_input_char\": 10000,\n",
    "    \"prompt_tpextraction\": \"prompts/prompt_tpextraction.txt\",\n",
    "    \"prompt_fusion\": \"prompts/prompt_fusion.txt\",\n",
    "    \"gold_concept_file\": \"\",\n",
    "    \"refined_concepts_file\": \"True\",\n",
    "    \"annotated_graph_file\": \"data/prerequisite_of_graph.tsv\",\n",
    "    \"language\": \"english\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f320654",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'language' not in config:\n",
    "    config['language'] = \"english\"\n",
    "if 'gold_concept_file' not in config:\n",
    "    config['gold_concept_file'] = \"\"\n",
    "\n",
    "# create BERTopic Extractor\n",
    "# language dependent part\n",
    "if config['language'] == \"english\":\n",
    "    vectorizer_model = CountVectorizer(ngram_range=(2, 4),\n",
    "                                        stop_words=\"english\")\n",
    "    sentence_model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "else:\n",
    "    print(f\"Using language {config['language']}.\")\n",
    "    print(\"Language not yet supported. Exiting.\")\n",
    "    exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e619396c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file: Dsa_clean.txt\n"
     ]
    }
   ],
   "source": [
    "texts = []\n",
    "for file in os.listdir(f'test/raw/'):\n",
    "    if file.endswith('.txt'):\n",
    "        print(f\"Loading file: {file}\")\n",
    "        with open(f'test/raw/{file}', 'r') as f:\n",
    "            for line in f:\n",
    "                texts.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8d57ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-01 13:04:02,606 - BERTopic - Embedding - Transforming documents to embeddings.\n",
      "Batches: 100%|██████████| 114/114 [00:08<00:00, 13.81it/s]\n",
      "2025-09-01 13:04:10,904 - BERTopic - Embedding - Completed ✓\n",
      "2025-09-01 13:04:10,904 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-09-01 13:04:31,713 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-09-01 13:04:31,716 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-09-01 13:04:31,961 - BERTopic - Cluster - Completed ✓\n",
      "2025-09-01 13:04:31,961 - BERTopic - Representation - Extracting topics using c-TF-IDF for topic reduction.\n",
      "2025-09-01 13:04:32,042 - BERTopic - Representation - Completed ✓\n",
      "2025-09-01 13:04:32,042 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2025-09-01 13:04:32,052 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-09-01 13:04:38,378 - BERTopic - Representation - Completed ✓\n",
      "2025-09-01 13:04:38,381 - BERTopic - Topic reduction - Reduced number of topics from 92 to 50\n"
     ]
    }
   ],
   "source": [
    "# language independent part\n",
    "umap_model = UMAP(n_neighbors=20, n_components=50, metric=\"cosine\", min_dist=0.0, random_state=37)\n",
    "ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=False)\n",
    "representation_model = KeyBERTInspired()\n",
    "\n",
    "topic_model = BERTopic(verbose=True,\n",
    "                        umap_model=umap_model,\n",
    "                        ctfidf_model=ctfidf_model,\n",
    "                        vectorizer_model=vectorizer_model,\n",
    "                        embedding_model=sentence_model,\n",
    "                        representation_model=representation_model,\n",
    "                        nr_topics=50,\n",
    "                        low_memory=True,\n",
    "                        calculate_probabilities=False)\n",
    "\n",
    "topics, _ = topic_model.fit_transform(texts)\n",
    "all_topics = topic_model.get_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11d79e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_concepts = []\n",
    "for topic_num, keywords in all_topics.items():\n",
    "    if topic_num != -1:\n",
    "        topic_keywords = [word for word, value in keywords]\n",
    "        extracted_concepts.extend(topic_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd529773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concepts written to output/concepts.tsv.\n"
     ]
    }
   ],
   "source": [
    "# remove duplicates\n",
    "extracted_concepts = list(set(keyword.lower() for keyword in extracted_concepts))\n",
    "\n",
    "if not os.path.exists('output'):\n",
    "    os.makedirs('output')\n",
    "\n",
    "# write extracted concepts to file\n",
    "with open('output/concepts.tsv', \"w\") as f:\n",
    "    for id, concept in enumerate(extracted_concepts, 1):\n",
    "        f.write(f\"{id}|{concept}\\n\")\n",
    "print(f\"Concepts written to output/concepts.tsv.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04480d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def singularize_concept(concept):\n",
    "    words = concept.split()\n",
    "    singular_words = [lemmatizer.lemmatize(word, wordnet.NOUN) for word in words]\n",
    "    return ' '.join(singular_words)\n",
    "\n",
    "# singularize concepts\n",
    "extracted_concept = [singularize_concept(concept) for concept in extracted_concepts]\n",
    "\n",
    "df_concepts = pd.DataFrame(extracted_concept, columns=[\"concept\"])\n",
    "df_concepts[\"label\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510906bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing concepts: 100%|██████████| 450/450 [03:09<00:00,  2.38it/s]\n"
     ]
    }
   ],
   "source": [
    "df_concepts = df_concepts.drop_duplicates(subset=\"concept\", keep=\"first\")\n",
    "\n",
    "# reduce the text dataset to only texts containing the concepts\n",
    "def filter_abstracts_by_term(term, abstracts, threshold=70):\n",
    "    filtered_abstracts = []\n",
    "    for abstract in abstracts:\n",
    "        if isinstance(abstract, str):\n",
    "            if fuzz.partial_ratio(term.lower(), abstract.lower()) >= threshold:\n",
    "                filtered_abstracts.append(abstract)\n",
    "    return filtered_abstracts\n",
    "\n",
    "concept_abstracts = {}\n",
    "for index, row in tqdm(df_concepts.iterrows(), desc=\"Processing concepts\",\n",
    "                        total=df_concepts.shape[0]):\n",
    "    concept = row[\"concept\"]\n",
    "    label = row[\"label\"]\n",
    "    filtered_abstracts = filter_abstracts_by_term(concept, texts)\n",
    "    concept_abstracts[concept] = {\n",
    "        \"abstracts\": filtered_abstracts,\n",
    "        \"label\": label\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8dc0a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output/concept_text.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(concept_abstracts, f, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "-1.-1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
